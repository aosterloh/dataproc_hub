{"cells": [{"cell_type": "markdown", "metadata": {"id": "E1-2NGnBscJo"}, "source": "## 4. Data Ops - Deploy Spark pipeline using Dataproc Workflows\n\n### This notebook looks at automation using Dataproc workflows\n\nDataproc Workflows has 2 types of workflow templates.\n\n1. Manged cluster - Create a new cluster and delete the cluster once the job has completed.\n2. Cluster selector - Select a pre-existing Dataproc cluster to the run the jobs (does not delete the cluster).\n\nThis module will use option 1 to create a managed cluster workflow template."}, {"cell_type": "markdown", "metadata": {"id": "RYHGVNfTscJo"}, "source": "### 4.1 Write pyspark jobs for dataproc workflow\n\n* Job 1: To convert CSV to BQ Tables\n* Job 2: Run predictions on trained model and persist results\n\nResources:\n* Learn more about dataproc workflows [[here]](https://cloud.google.com/dataproc/docs/concepts/workflows/overview)\n* Learn more about adding pyspark jobs to a worfklow template [[here]](https://cloud.google.com/sdk/gcloud/reference/dataproc/workflow-templates/add-job/pyspark)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Setup BQ tables for persisiting results from pyspark jobs"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# Let's create bq tables to persist the results of the jobs\nproject_id = !gcloud config list --format 'value(core.project)' 2>/dev/null \nproject_id = project_id[0]\n# Table definition for both jobs \njob1_table_name = project_id + '-raw.transaction_data_workflow'\njob2_table_name = project_id + '-annotated.transaction_data_workflow'\njob1_table_name = job1_table_name.replace('-', '_')\njob2_table_name = job2_table_name.replace('-', '_')"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "BigQuery error in mk operation: Table\n'thatistoomuchdata:thatistoomuchdata_raw.transaction_data_workflow' could not be\ncreated; a table with this name already exists.\n"}], "source": "schema_inline = 'step:int64,type:string,amount:float64,oldbalanceOrg:float64,newbalanceOrig:float64,oldbalanceDest:float64,newbalanceDest:float64,isFraud:int64,transactionID:string'\n!bq mk --table \\\n{job1_table_name} \\\n{schema_inline}"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "BigQuery error in mk operation: Table\n'thatistoomuchdata:thatistoomuchdata_annotated.transaction_data_workflow' could\nnot be created; a table with this name already exists.\n"}], "source": "schema_inline = 'step:int64,type:string,amount:float64,oldbalanceOrg:float64,newbalanceOrig:float64,oldbalanceDest:float64,newbalanceDest:float64,isFraud:int64,transactionID:string,type_Index:float64,type_OHE:STRING,features:STRING,rawPrediction:STRING,probability:STRING,prediction:float64'\n!bq mk --table \\\n{job2_table_name} \\\n{schema_inline}"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Define Job 1 (Read csv and persist in BQ) \n\n**TODO**\n* Provide project_id and train_data_path "}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "DCQwAUOfscJp"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing job_csv_to_bq_table.py\n"}], "source": "%%writefile job_csv_to_bq_table.py\n## Job 1\nprint('Job 1')\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n.appName('Automated Data Engineer Workflow') \\\n.config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\") \\\n.getOrCreate()\n\n# variables\nproject_id = 'thetraining-project'\ngcs_bucket = project_id + '-data'\nBUCKET_PATH= 'gs://'+ gcs_bucket\ntrain_data_path = BUCKET_PATH+ '/transaction_data_train.csv'\ntable_name = project_id + '-raw.transaction_data_workflow'\ntable_name = table_name.replace('-', '_')\n\ndf_transaction_data_from_csv = spark \\\n  .read \\\n  .option (\"inferSchema\" , \"true\") \\\n  .option (\"header\" , \"true\") \\\n  .csv (train_data_path)\n\ndf_transaction_data_from_csv.write \\\n.format(\"bigquery\") \\\n.option(\"table\", table_name) \\\n.option(\"temporaryGcsBucket\", gcs_bucket) \\\n.mode('overwrite') \\\n.save()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Define Job 2 (Run predictions on trained model and persist results)"}, {"cell_type": "markdown", "metadata": {}, "source": "##### **TODO** (Challenge 1)\n* Provide project_id and train_data_path variables \n* Provide code to: \n    * load test data into a pyspark dataframe \n    * load previously trained ML model \n    * run prediction on test data \n    * persist relevant fields into BQ table"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing job_ml_predictions.py\n"}], "source": "%%writefile job_ml_predictions.py\nprint('Job 2')\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import PipelineModel\n\nproject_id = 'thetraining-project'\ngcs_bucket = project_id + '-data'\nBUCKET_PATH= 'gs://'+ gcs_bucket\n\npath_to_predict_csv = BUCKET_PATH+ '/transaction_data_test.csv'\nmodel_path = BUCKET_PATH + '/model/'\ntable_name = project_id + '-annotated.transaction_data_workflow'\ntable_name = table_name.replace('-', '_')\n\nspark = SparkSession.builder \\\n.appName('Automated Data Scientist Workflow') \\\n.config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\") \\\n.getOrCreate()\n\n# Load the test data into a spark dataframe \ndf_test_data = spark \\\n  .read \\\n  .option (\"inferSchema\" , \"true\") \\\n  .option (\"header\" , \"true\") \\\n  .csv (path_to_predict_csv)\n# Load the previously trained pipeline model \nloaded_pipeline_model = PipelineModel.load(model_path)\n# Run predictions on the test data\npredictions = loaded_pipeline_model.transform(df_test_data)\n\n# # Save the following fields into BQ table: 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'isFraud', 'transactionID', 'prediction' \n# # Use the bq table name defined previously {table_name}\npredictions.write \\\n.format(\"bigquery\") \\\n.option(\"table\", table_name) \\\n.option(\"temporaryGcsBucket\", gcs_bucket) \\\n.mode('overwrite') \\\n.save()"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "markdown", "metadata": {"id": "g2fL7Z5iscJr"}, "source": "### 4.2 Grant additional service account permission to deploy workflow from notebooks\n\nGo to https://console.cloud.google.com/iam-admin/iam\n\nLook for Compute Engine default service account. It will be in the format\n\n```\n{number}-compute@developer.gserviceaccount.com\n```\n\nEdit the roles and add the role \"Storage Object Admin\" and press save.\n\nNote that these steps are taken to deploy the workflow directly from the notebooks. Alternatively you can execute the steps below via cloud shell. "}, {"cell_type": "markdown", "metadata": {"id": "QC7qXjdMscJt"}, "source": "### 4.3 Create Dataproc managed cluster workflow Template\n\nLearn more about creating workflow templates [[here]](https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#creating_a_template)\n\n**TODO**\n* Provide values for variables below"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": "# %%bash\n# gcloud dataproc workflow-templates delete automate-bank-transaction-workflow --region=europe-west3"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": "# %%bash\n# gsutil rm gs://thetraining-project-data/workflows/*"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "env: WORKFLOW_ID=automate-bank-transaction-workflow-west1-ak\nenv: REGION=europe-west1\nenv: PROJECT_ID=thetraining-project\nenv: BUCKET_NAME=thetraining-project-data\nenv: CLUSTER_NAME=spark-workflow-cluster\nenv: NUM_WORKERS=2\n"}], "source": "%env WORKFLOW_ID=automate-bank-transaction-workflow-west1-ak\n%env REGION=europe-west1\n%env PROJECT_ID=thetraining-project\n%env BUCKET_NAME= thetraining-project-data\n%env CLUSTER_NAME=spark-workflow-cluster\n%env NUM_WORKERS=2"}, {"cell_type": "code", "execution_count": 33, "metadata": {"id": "GUQ7vcnuscJw"}, "outputs": [], "source": "%%bash\ngcloud dataproc workflow-templates create $WORKFLOW_ID \\\n--region $REGION"}, {"cell_type": "markdown", "metadata": {"id": "S8BqGX5VscJw"}, "source": "### 4.4 Configure managed cluster for the workflow template"}, {"cell_type": "code", "execution_count": 34, "metadata": {"id": "YVl2u2mSscJx"}, "outputs": [], "source": "%%bash\ngcloud beta dataproc workflow-templates set-managed-cluster $WORKFLOW_ID \\\n    --cluster-name $CLUSTER_NAME \\\n    --region $REGION \\\n    --image-version=1.5-ubuntu18 \\\n    --master-machine-type n1-standard-2 \\\n    --master-boot-disk-size=128GB \\\n    --num-workers $NUM_WORKERS \\\n    --worker-machine-type n1-standard-2\\\n    --worker-boot-disk-size=128GB \\\n    --scopes https://www.googleapis.com/auth/cloud-platform \\\n    --bucket $BUCKET_NAME \\\n    --properties spark:spark.jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.18.0.jar,spark:spark=gs://spark-lib/bigquery/spark-bigquery-latest.jar"}, {"cell_type": "markdown", "metadata": {"id": "ZG53bc48scJy"}, "source": "### 4.5 Upload PySpark job to GCS"}, {"cell_type": "code", "execution_count": 35, "metadata": {"id": "_O0wO8H7scJz"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Copying file://job_csv_to_bq_table.py [Content-Type=text/x-python]...\n/ [1 files][  921.0 B/  921.0 B]                                                \nOperation completed over 1 objects/921.0 B.                                      \n"}], "source": "%%bash\ngsutil cp job_csv_to_bq_table.py \\\n gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py"}, {"cell_type": "code", "execution_count": 36, "metadata": {"id": "6_ppc1GfscJz"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Copying file://job_ml_predictions.py [Content-Type=text/x-python]...\n/ [1 files][  1.4 KiB/  1.4 KiB]                                                \nOperation completed over 1 objects/1.4 KiB.                                      \n"}], "source": "%%bash\ngsutil cp job_ml_predictions.py \\\n gs://${PROJECT_ID}-data/workflows/python-scripts/job_ml_predictions.py"}, {"cell_type": "markdown", "metadata": {"id": "2DX87wdpscJ0"}, "source": "### 4.6 Add job to workflow template"}, {"cell_type": "code", "execution_count": 37, "metadata": {"id": "lukmzJbMscJ0"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "createTime: '2021-03-17T15:31:07.904Z'\nid: automate-bank-transaction-workflow-west1-ak\njobs:\n- pysparkJob:\n    mainPythonFileUri: gs://thetraining-project-data/workflows/python-scripts/job_csv_to_bq_table.py\n  stepId: csv_to_bq\nname: projects/thetraining-project/regions/europe-west1/workflowTemplates/automate-bank-transaction-workflow-west1-ak\nplacement:\n  managedCluster:\n    clusterName: spark-workflow-cluster\n    config:\n      configBucket: thetraining-project-data\n      gceClusterConfig:\n        serviceAccountScopes:\n        - https://www.googleapis.com/auth/cloud-platform\n      masterConfig:\n        diskConfig:\n          bootDiskSizeGb: 128\n        machineTypeUri: n1-standard-2\n      softwareConfig:\n        imageVersion: 1.5-ubuntu18\n        properties:\n          spark:spark: gs://spark-lib/bigquery/spark-bigquery-latest.jar\n          spark:spark.jars: gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.18.0.jar\n      workerConfig:\n        diskConfig:\n          bootDiskSizeGb: 128\n        machineTypeUri: n1-standard-2\n        numInstances: 2\nupdateTime: '2021-03-17T15:31:17.671Z'\nversion: 3\n"}], "source": "%%bash\ngcloud dataproc workflow-templates add-job pyspark \\\n   gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py \\\n    --region $REGION \\\n    --step-id csv_to_bq \\\n    --workflow-template $WORKFLOW_ID"}, {"cell_type": "code", "execution_count": 38, "metadata": {"id": "sdtd-QZpscJ1"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "createTime: '2021-03-17T15:31:07.904Z'\nid: automate-bank-transaction-workflow-west1-ak\njobs:\n- pysparkJob:\n    mainPythonFileUri: gs://thetraining-project-data/workflows/python-scripts/job_csv_to_bq_table.py\n  stepId: csv_to_bq\n- prerequisiteStepIds:\n  - csv_to_bq\n  pysparkJob:\n    mainPythonFileUri: gs://thetraining-project-data/workflows/python-scripts/job_ml_predictions.py\n  stepId: predict\nname: projects/thetraining-project/regions/europe-west1/workflowTemplates/automate-bank-transaction-workflow-west1-ak\nplacement:\n  managedCluster:\n    clusterName: spark-workflow-cluster\n    config:\n      configBucket: thetraining-project-data\n      gceClusterConfig:\n        serviceAccountScopes:\n        - https://www.googleapis.com/auth/cloud-platform\n      masterConfig:\n        diskConfig:\n          bootDiskSizeGb: 128\n        machineTypeUri: n1-standard-2\n      softwareConfig:\n        imageVersion: 1.5-ubuntu18\n        properties:\n          spark:spark: gs://spark-lib/bigquery/spark-bigquery-latest.jar\n          spark:spark.jars: gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.18.0.jar\n      workerConfig:\n        diskConfig:\n          bootDiskSizeGb: 128\n        machineTypeUri: n1-standard-2\n        numInstances: 2\nupdateTime: '2021-03-17T15:31:19.086Z'\nversion: 4\n"}], "source": "%%bash\ngcloud dataproc workflow-templates add-job pyspark \\\n  gs://${PROJECT_ID}-data/workflows/python-scripts/job_ml_predictions.py \\\n    --region $REGION \\\n    --start-after=csv_to_bq \\\n    --step-id predict \\\n    --workflow-template $WORKFLOW_ID"}, {"cell_type": "markdown", "metadata": {"id": "_ug2dKqWscJ2"}, "source": "### 4.7 Run workflow template"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "d5UY8epDscJ2", "outputId": "5d09224f-4790-4ece-f689-23d604017baf"}, "outputs": [], "source": "%%bash\ngcloud dataproc workflow-templates instantiate $WORKFLOW_ID \\\n--region $REGION"}, {"cell_type": "markdown", "metadata": {"id": "owS3gXdHscJ4"}, "source": "### View Cluster, workflow and jobs tabs\n\nGo to the Dataproc UI and view the cluster page. You should see the new cluster spinning up\n\nOnce the cluster is ready view the workflow and jobs tabs to check the progress of the jobs."}, {"cell_type": "markdown", "metadata": {"id": "pDhN2ph6scJ6"}, "source": "### 4.8 Schedule the workflows\n**TODO** (Optional: Challenge 2)\n* Cloud Composer is a managed Apache Airflow service you can use to create, schedule, monitor, and manage workflows.\n* View the guide on how to schedule Dataproc workflows in composer [[here]](https://cloud.google.com/dataproc/docs/tutorials/workflow-composer)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"colab": {"name": "Spark - Bank Marketing Demo (1).ipynb", "provenance": []}, "kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}